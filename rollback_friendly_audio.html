<html>
<head>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Rollback Friendly Audio&#8211Jacob Bell's Blog</title>
	<link rel="stylesheet" type="text/css" href="style.css">
	<link rel="shortcut icon" href="favicon.png">
</head>
<body>
	<div class="content">
		<div class="header">
			<a href="index.html"><img src="avatar.png"><span>Jacob Bell's Blog</span></a>
		</div>
		<div class="post">
			<h1>Rollback Friendly Audio</h1>
			<div class="date">
				30 January 2023
			</div>
			<p>For an overview of rollback netcode, watch <a href="https://www.youtube.com/watch?v=0NLe4IpdS1w">Core-A Gaming's video on it</a>.</p>
			<p>Rollback netcode requires that game state can be duplicated. Each frame needs to be saved into a history buffer so the game can go back a few frames and resimulate when an input arrives late and is different than expected. For example, if the opponent's input for frame 2 arrives on frame 6, we go back in time and resimulate frames 2, 3, 4, and 5 with the new information, then simulate and render frame 6. This is easy if the game state is made up of statically sized arrays and no pointers that reference memory inside the game state:</p>
<pre class="code">struct EntityHandle {
	int id;
	int index;
};

struct Entity {
	Vector3 position;
	int state;
	EntityHandle target;
};

struct GameState {
	Entity entities[64]
	int randomSeries;
};</pre>
			<p>It's also easy with a pure functional style of game update that always returns a new game state instead of changing the old one.</p>
<pre class="code">frames[n+1] = updateGameState(frames[n]);</pre>
			<p>Things get complicated quickly when pointer-filled game state is being updated through modifications. Michael Stallone did a great <a href="https://www.youtube.com/watch?v=7jb0FOcImdg">GDC talk on the difficulty of integrating rollback netcode into Netherrealms Studio's object-oriented game engine</a>.</p>
			<p>Only state that changes depending on inputs needs to be stored for each frame. The game will always catch back up to real-time after a rollback, so things like a waterfall's animation in the background can seamlessly ignore rollback. There probably won't be big enough differences before and after rollback to warrent re-rendering every in-between frame for temporal graphics effects like TAA.</p>
			<p>Audio is a different challenge, with a dependency on player input and running asynchronously with gameplay. Starting and stopping sounds requires separate timers from the ususal game time step. Games usually handle this just fine, but rollback adds an unusual requirement to an audio system: changing what sounds started playing in the past. Not being able to cancel a sound that a rollback determined should not have happened would be very distracting.</p>
			<div class="picture"><img src="rollback_friendly_audio/knock_out.png"><p>In early versions of King of Fighters 15, sounds would continue to play after a rollback. Especially noticible if a K.O. was rolled back.</p></div>
			<p>I felt like most of my game engine components would work well if I decided to make a game with rollback, but realized I had no idea how to do the audio. I wanted to replace DirectSound and <a href="https://developercommunity.visualstudio.com/t/asan-x64-causes-unhandled-exception-at-0x00007ffac/1365655">its weird bugs</a> anyway, so I began thinking about how to make my ideal audio system.</p>
			<p>There were four axes along which I wanted to optimize audio for rollback:</p>
			<ul>
				<li>Latency: speakers should begin playing audio right after a sound is triggered in game.</li>
				<li>Performance: avoid doing unnecessary work on the CPU.</li>
				<li>Rollbackability: it should be possible to go back back in time to a previous frame's audio state and play sounds at the correct time while fast-forwarding.</li>
				<li>Simplicity: the code should be clear and easy to understand. Reduce the chance of memory bugs and have no possibility of race conditions.</li>
			</ul>

			<h2>Playing Audio</h2>
			<p>To output audio, we ask the operating system for a buffer and fill it with audio samples. There are various levels of abstraction to do this, but the lowest level thing on Windows is WASAPI.</p>
			<p>get a buffer, we need a handle to an audio device, such as headphones or a laptop's built-in speakers. To get a device, we need an enumerator. This can list all of the audio devices connected to the system, but WASAPI's has a convenient method to get the user's primary device, or "default endpoint."</p>
			<p>With a device we can create a client with a specific format, and a render-client that actually gives us the buffer for writting samples. In my game I use floating point for samples in the [0, 1] range, since it offers plenty of overhead when mixing and makes the math simple.</p>
			<p>The client's period hints how long we want the buffer to be. This directly impacts latency.</p>
			<p>With everything set up, we can write audio samples in a loop. First we get the "padding," which indicates how much of the buffer is unplayed. We then get a pointer and write to the played region of the buffer. The following example program shows all these steps and outputs a sine wave.</p>
			<details class="code">
				<summary>
						<li></li>minimal_wasapi.cpp<a href="rollback_friendly_audio/minimal_wasapi.cpp">raw</a>
				</summary>
<pre><span class="comment">// Compile: cl minimal.cpp /link ole32.lib</span>
#include &lt;windows.h&gt;
#include &lt;inttypes.h&gt;
#include &lt;math.h&gt;
#include &lt;mmdeviceapi.h&gt;
#include &lt;audioclient.h&gt;

#define BYTES_PER_CHANNEL sizeof(float)
#define CHANNELS_PER_SAMPLE 1
#define SAMPLES_PER_SECOND 44100

int main()
{
	CoInitializeEx(nullptr, COINIT_DISABLE_OLE1DDE);

	IMMDeviceEnumerator* enumerator;
	CoCreateInstance(__uuidof(MMDeviceEnumerator), NULL, CLSCTX_ALL, __uuidof(IMMDeviceEnumerator), (void**)&enumerator);

	IMMDevice* endpoint;
	enumerator-&gt;GetDefaultAudioEndpoint(eRender, eMultimedia, &endpoint);

	IAudioClient* audioClient;
	endpoint-&gt;Activate(__uuidof(IAudioClient), CLSCTX_ALL, NULL, (void**)&audioClient);

	REFERENCE_TIME defaultDevicePeriod, minDevicePeriod;
	audioClient-&gt;GetDevicePeriod(&defaultDevicePeriod, &minDevicePeriod);

	WAVEFORMATEX format = {0};
	format.wFormatTag = WAVE_FORMAT_IEEE_FLOAT;
	format.nChannels = CHANNELS_PER_SAMPLE;
	format.nSamplesPerSec = SAMPLES_PER_SECOND;
	format.wBitsPerSample = BYTES_PER_CHANNEL*8;
	format.nBlockAlign = (CHANNELS_PER_SAMPLE*BYTES_PER_CHANNEL);
	format.nAvgBytesPerSec = SAMPLES_PER_SECOND*format.nBlockAlign;
	audioClient-&gt;Initialize(AUDCLNT_SHAREMODE_SHARED, AUDCLNT_STREAMFLAGS_AUTOCONVERTPCM, defaultDevicePeriod, 0, &format, 0);

	IAudioRenderClient* renderClient;
	audioClient-&gt;GetService(__uuidof(IAudioRenderClient), (void**)&renderClient);

	uint32_t bufferSampleCount;
	audioClient-&gt;GetBufferSize(&bufferSampleCount);

	audioClient-&gt;Start();
	uint64_t sampleTime = 0;

	while (1)
	{
		uint32_t paddingSampleCount; 
		audioClient-&gt;GetCurrentPadding(&paddingSampleCount);

		uint32_t sampleCount = bufferSampleCount - paddingSampleCount;
		float* buffer;
		if (SUCCEEDED(renderClient-&gt;GetBuffer(sampleCount, (BYTE**)&buffer)))
		{
			<span class="comment">// Sine wave</span>
			float toneHerz = 500;
			for (uint32_t i=0; i&lt;sampleCount; ++i) {
				buffer[i] = sinf(sampleTime*3.14159f*toneHerz/SAMPLES_PER_SECOND);
				sampleTime += 1;
			}
			renderClient-&gt;ReleaseBuffer(sampleCount, 0);
		}
	}
}</pre></details>
			<p>We need to write to the buffer frequently enough that it won't run out of audio to play. For a buffer of 1000 samples being played at 44100 samples per second, we would need to write every frame at 60fps. Any slower than that and the player could hear drops in the audio.</p>
			<p>Games often have periods of stuttering slowdown on even the latest hardware, but audio ususally continues playing smoothly. We need a better strategy than expecting to run at a perfect frame rate. One solution would be increasing the size of the buffer. For example, a 50ms buffer would survive down to 20fps. The downside is this introduces latency: the audio data the game outputs will start playing up to 50ms in the future.</p>
			<p>A small audio buffer optimizes for low latency. To write to a small buffer frequently, most games use a dedicated audio thread. The OS can wake the thread when the audio buffer is low on unplayed samples.</p>
			<p>Following is mostly the same example, but this time playing on a separate thread and using an event to wake it.</p>
			<details class=code>
				<summary>
						<li></li>audio_thread.cpp<a href="rollback_friendly_audio/audio_thread.cpp">raw</a>
				</summary>
<pre><span class=comment>// Compile: cl audio_thread.cpp /link ole32.lib</span>
#include &ltwindows.h&gt;
#include &ltinttypes.h&gt;
#include &ltmath.h&gt;
#include &ltmmdeviceapi.h&gt;
#include &ltaudioclient.h&gt;

#define BYTES_PER_CHANNEL sizeof(float)
#define CHANNELS_PER_SAMPLE 1
#define SAMPLES_PER_SECOND 44100

DWORD WINAPI audioThread(void* param)
{
	CoInitializeEx(nullptr, COINIT_DISABLE_OLE1DDE);
	HRESULT hr;
	IMMDeviceEnumerator* enumerator;
	hr = CoCreateInstance( __uuidof(MMDeviceEnumerator), NULL, CLSCTX_ALL, __uuidof(IMMDeviceEnumerator), (void**)&enumerator);

	IMMDevice* endpoint;
	hr = enumerator-&gt;GetDefaultAudioEndpoint(eRender, eMultimedia, &endpoint);

	IAudioClient* audioClient;
	hr = endpoint-&gt;Activate(__uuidof(IAudioClient), CLSCTX_ALL, NULL, (void**)&audioClient);

	REFERENCE_TIME defaultDevicePeriod, minDevicePeriod;
	hr = audioClient-&gt;GetDevicePeriod(&defaultDevicePeriod, &minDevicePeriod);

	WAVEFORMATEX format = {0};
	format.wFormatTag = WAVE_FORMAT_IEEE_FLOAT;
	format.nChannels = CHANNELS_PER_SAMPLE;
	format.nSamplesPerSec = SAMPLES_PER_SECOND;
	format.wBitsPerSample = BYTES_PER_CHANNEL*8;
	format.nBlockAlign = (CHANNELS_PER_SAMPLE*BYTES_PER_CHANNEL);
	format.nAvgBytesPerSec = SAMPLES_PER_SECOND*format.nBlockAlign;
	hr = audioClient-&gt;Initialize(AUDCLNT_SHAREMODE_SHARED, AUDCLNT_STREAMFLAGS_EVENTCALLBACK | AUDCLNT_STREAMFLAGS_AUTOCONVERTPCM, defaultDevicePeriod, 0, &format, 0);

	HANDLE wakeAudioThreadEvent = CreateEventA(0, false, false, 0);
	hr = audioClient-&gt;SetEventHandle(wakeAudioThreadEvent);

	IAudioRenderClient* renderClient;
	hr = audioClient-&gt;GetService(__uuidof(IAudioRenderClient), (void**)&renderClient);

	uint32_t bufferSampleCount;
	hr = audioClient-&gt;GetBufferSize(&bufferSampleCount);

	uint64_t sampleTime = 0;
	hr = audioClient-&gt;Start();

	while (WaitForSingleObject(wakeAudioThreadEvent, INFINITE) == WAIT_OBJECT_0)
	{
		uint32_t paddingSampleCount; 
		audioClient-&gt;GetCurrentPadding(&paddingSampleCount);

		uint32_t sampleCount = bufferSampleCount - paddingSampleCount;
		float* buffer;
		if (SUCCEEDED(renderClient-&gt;GetBuffer(sampleCount, (BYTE**)&buffer)))
		{
			<span class=comment>// Sine wave</span>
			float toneHerz = 500;
			for (uint32_t i=0; i&lt;sampleCount; ++i) {
				buffer[i] = sinf(sampleTime*3.14159f*toneHerz/SAMPLES_PER_SECOND);
				sampleTime += 1;
			}
			renderClient-&gt;;ReleaseBuffer(sampleCount, 0);
		}
	}
	return 0;
}

void main()
{
	CreateThread(0, 0, audioThread, 0, 0, 0);
	Sleep(INFINITE);
}</pre></details>

			<h2>Playing Waveforms</h2>
			<p>Sounds and music played in a game are stored as a series of samples, making a waveform. To play sounds we zero out the audio output buffer then sum up waveform samples into it, called mixing.</p>
			Having an audio thread makes accessing waveform assets complicated. Audio in games is always playing, even when loading and unloading assets. We need to be sure that the audio thread is not trying to write out a waveform while that waveform is being unloaded.</p>
			<p>One way to approach this problem is to mix sounds into an output buffer on the gameplay thread, avoiding multithreaded access of assets entirely. Then we have the problem mentioned before with inconsistent framerates causing audio glitches. We can mitigate it by writing more audio than necessary per frame. If the gameplay thread mixes 100ms worth of audio instead of 16ms worth, the game could drop to 10fps before audio starts dropping.</p>
			<p>To prevent this from introducing a lot of latency, there are a few strategies we could use:</p>
			<div class="picture"><img src="rollback_friendly_audio/double_buffer.png" alt="Double buffer"><p>Mix in the gameplay thread and exchange buffer pointers with the audio thread when finished.</p></div>
			<div class="picture"><img src="rollback_friendly_audio/ring_buffer.png" alt="Shared ring buffer"><p>A play-head marks the where the audio thread is reading from and a write-head marks where mixed audio can be written to. The write-head stays just in front of the play-head. This is how it's done in DirectSound, a Windows API that wraps WASAPI.</p></div>
			<p>With either method, the audio thread copies the mixed audio into the OS's output buffer. The downside is we're mixing more audio than we need to. If the game is currently running at 16ms per frame, that's 84ms worth of audio that goes unused. On the other hand, computers are extreamly fast, and the wasted work might make no difference if the game is already well within its CPU budget.</p>
			<p>Since one of my goals was to optimize for CPU usage, I wanted to only mix audio that would actually be played. To do that, I needed to mix on the audio thread.</p>

			<h2>Managing Assets</h2>
			<p>When multithreading, I don't want to just say "make sure to stop all playing sounds before unloading them." I want to be sure that race conditions will never happen.</p>
			<p>With audio mixing on its own thread, rendering audio becomes similar to rendering graphics. OpenGL, for instance, gives user code handles to buffers that are managed on the GPU side of things. We could do the same with audio, giving the audio thread control over waveform lifetimes, while the gameplay thread refers to them using handles.</p>
			<p>The multithreaded operations I needed were</p>
			<ul>
				<li>Add a waveform asset and return a handle to it.</li>
				<li>Mark a waveform asset for deletion that corresponds to a handle.</li>
				<li>Get the waveform asset that corresponds to a handle.</li>
			</ul>
			<p>Waveforms are loaded by the gameplay thread or by dedicated asset loader threads. Either way, adding an asset must be thread-safe. Marking for deletion must be as well, but the audio thread can actually free the memory synchronously after it's submitted the output buffer. In my engine, there's no way to have a handle on the gameplay thread after marking it for deletion, so temporarily getting the read-only asset is not a concern.</p>
			<p>One additional thing to consider is that the audio thread should finish mixing as soon as possible. After the OS wakes it up, we don't want to pass control back to the OS by waiting on a mutex or similar synchronization construct.</p>
			<p>By imposing an upper limit on how many waveforms can be loaded at once, it's easy to implement a thread-safe waveform manager using <a href="https://learn.microsoft.com/en-us/windows/win32/sync/interlocked-variable-access">lock-free atomics.</a></p>
			<details class="code">
				<summary>
						<li></li>waveform_manager.cpp<a href="rollback_friendly_audio/waveform_manager.cpp">raw</a>
				</summary>
<pre>#include &lt;windows.h&gt;
#include &lt;inttypes.h&gt;
#include &lt;assert.h&gt;

typedef int16_t MonoSample;

struct StereoSample
{
	int16_t left;
	int16_t right;
};

struct Waveform
{
	uint32_t sampleCount;
	MonoSample* monoSamples;
	StereoSample* stereoSamples;
};

struct WaveformHandle
{
	uint64_t id;
	uint32_t index;
};

struct WaveformManager
{
	static const uint32_t maxSlots = 1024;
	struct Slot {
		Waveform* waveform;
		volatile bool remove;
		volatile uint64_t id;
	};

	volatile Slot slots[maxSlots];
	volatile uint64_t previousWaveformId;
};

void destroyWaveform(Waveform* waveform)
{
	free(waveform-&gt;monoSamples);
	free(waveform-&gt;stereoSamples);
}

WaveformHandle addWaveform(WaveformManager* waveformManager, Waveform waveform)
{
	Waveform* waveformPtr = (Waveform*)malloc(sizeof(Waveform));
	memcpy(waveformPtr, &waveform, sizeof(Waveform));
	for (uint32_t i=0; i&lt;WaveformManager::maxSlots; ++i) {
		void* result = InterlockedCompareExchangePointer((void**)&waveformManager-&gt;slots[i].waveform, waveformPtr, 0);
		if (result == 0) {
			WaveformHandle handle = {0};
			handle.id = InterlockedIncrement(&waveformManager-&gt;previousWaveformId);
			handle.index = i;
			waveformManager-&gt;slots[i].id = handle.id;
			return handle;
		}
	}
	assert(!"Waveform manager is full; exceeded max loaded waveforms.");
	return {0};
}

Waveform* getWaveform(WaveformManager* waveformManager, WaveformHandle handle)
{
	return (handle.index &lt; waveformManager-&gt;maxSlots && waveformManager-&gt;slots[handle.index].id == handle.id)? waveformManager-&gt;slots[handle.index].waveform : 0;
}

void markWaveformForRemoval(WaveformManager* waveformManager, WaveformHandle handle)
{
	waveformManager-&gt;slots[handle.index].remove = true;
}

void updateWaveformManager(WaveformManager* waveformManager)
{
	for (uint32_t i=0; i&lt;WaveformManager::maxSlots; ++i) {
		if (waveformManager-&gt;slots[i].remove) {
			destroyWaveform(waveformManager-&gt;slots[i].waveform);
			free(waveformManager-&gt;slots[i].waveform);
			waveformManager-&gt;slots[i].waveform = 0;
		}
	}
}</pre>
			</details>

			<h2>Game Audio Interface</h2>
			<p>Audio libraries tend to have interfaces where you play sounds with a function: <span class="code">soundHandle = playSound(waveFile)</span>. Likewise, you would update and stop sounds by calling functions that take the sound handle.</p>
			<p>This means the list of playing sounds is managed by the audio library without a clear way to roll back to a previous state. When a rollback is needed, the game would have to tell the audio library to stop playing some sounds, roll back, and then start playing sounds again as we go through the frames.</p>
			<p>How would the audio library know at what point in time a sound started? Rollback cuts off the beginning of animations; it should also cut off the biginning of sounds so nothing is delayed.
			<br/>How could we identify which sounds started before the rollback so we don't lose those?
			<br/>How do we make sure sounds are re-played at the exact same point? Even one sample off is noticible.
			</p>
			<p>Answering these questions could get complicated, especially if an library is trying to be a blackbox. To optimize for simplicity, what we really need is a plain data structure of audio state that can be saved and duplicated.</p>
<pre class="code">struct Sound {
	AssetHandle waveFile;
	uint64_t startedSampleTime;
	float volume;
};

struct AudioState {
	Sound sounds[50];
};</pre>
			<p>Each frame has its own <span class="code">AudioState</span> that can be trivially copied. <span class="code">startedSampleTime</span> is when the individual sound should start playing. To set this, we need to know where in time the audio thread is currently rendering.</p>

			<h2>Time Sychronization</h2>
			<p>Gameplay tipically measures time in delta times and frames, which tend to be somewhat variable. Audio, hopefully, plays at a consistent rate no matter what else the game is dealing with. Audio time is measured in samples written and is updated every time we aquire the OS's audio buffer. I call this sample-time, and it counts up forever as long as the audio thread is running.</p>
			<p>Alongside the gamestate frame history, a history of sample-times can track when sounds were started per frame, so we can start playing sounds "in the past" durring rollback.</p>
			<div class="picture"><img src="rollback_friendly_audio/sample_times.png"></div>

			<h2>Transfering Data Between Gameplay Thread and Audio Thread</h2>
			<p>The sounds built up over a game update can be copied to a buffer for the audio thread. When the OS wakes the audio thread, it can update its sound state to match the buffer. It plays each sound from its <span class="code">startedSampleTime</span> relative to the audio thread's current sample-time.</p>
			<div class="picture"><img src="rollback_friendly_audio/transfer.png"></div>
			<p>In the other direction, the audio thread produces a sample-time that the gameplay thread uses to start sounds. To make these as close as possible, I set a sound's <span class="code">startedSampleTime</span> to zero when it's trigger while updating game state, then at the end of the update, get the audio thread's sample-time and set all sounds with zero <span class="code">startedSampleTime</span> to that. I add one period of delay to ensure the beginning won't be cut off.</p>

			<h2>Looping</h2>
			<p>Background music plays in a loop, usually with an intro portion that only plays once. I specify a loop sample index for each song that gets passed to the audio thread. When playing a looping sound past the end of the waveform, it goes back to the loop sample index.</p>
			<div class="picture"><img src="rollback_friendly_audio/loop.png"></div>

			<h2>Reverb</h2>
			<p>After mixing, I apply a <a href="https://gist.github.com/rxi/e5488c6660154329ddfc4a7a7d2997f8">reverb filter</a> to the OS output buffer for an echoing effect when in game areas like caves. The filter retains its state between uses, so each time it's applied to the audio buffer, reverb carries over from the last. In a more sophisticated game with 3D sound effects, or to prevent rolled-back sounds from continuing to reverberate, the filter could be applied to individual sounds instead of the whole mixed buffer.</p>
			<p>Calculating reverb is serial and relatively expensive. Adding it is what prompted me to switch from the method of filling a buffer with more mixed audio than needed to mixing on the audio thread.</p>

			<h2>Device Changes</h2>
			<p>With WASAPI, the audio client is tied to a device. When the user switches their primary device from headphones to speakers, the audio will continue playing on headphones. When the user unplugs their headphones, WASAPI functions will return error messages. In both cases I'd rather support what the user is trying to do.</p>
			<p>We can create an object extending the <span class="code">IMMNotificationClient</span> interface that listens for device change events. When an event occurs, I wake the audio thread, destroy all the old WASAPI objects, and recreate them with the new default device.</p>
			<p>One last problem is when there's no device at all. The game will continue queueing sounds but the audio thread will never wake to play them, and without updated sample-times the gameplay thread won't know when to end them. I wrote a special case for this to time out waiting on the audio thread every second, allowing the thread to advance sample-time without playing any sounds.</p>
			<p>This example program uses an audio thread to play a sine-wave song based on user input. It handles device changes and shows the WASAPI wrapper interface I made for my games.</p>
			<details class="code">
				<summary>
					<li></li>music_box.cpp<a href="rollback_friendly_audio/music_box.cpp">raw</a>
				</summary>
<pre><span class=comment>// Compile: cl music_box.cpp /link ole32.lib</span>
<span class=comment>// Usage:   music_box.exe "Hello World"</span>
#include &lt;windows.h&gt;
#include &lt;inttypes.h&gt;
#include &lt;math.h&gt;
#include &lt;mmdeviceapi.h&gt;
#include &lt;audioclient.h&gt;

#define BYTES_PER_CHANNEL sizeof(float)
#define CHANNELS_PER_SAMPLE 1
#define SAMPLES_PER_SECOND 44100

<span class=comment>// Public API -----------------------------------------------------------------</span>
struct AudioEvents;
struct AudioDevice;
struct AudioBuffer;
AudioEvents createAudioEvents ();
void        destroyAudioEvents(AudioEvents* events);
void        endAudioEvents    (AudioEvents* events);
AudioDevice createAudioDevice (AudioEvents* events);
void        destroyAudioDevice(AudioDevice* device);
bool        waitForAudioBuffer(AudioDevice* device, AudioEvents* events, AudioBuffer* out_buffer);
void        submitAudioBuffer (AudioDevice* device, AudioBuffer buffer);

<span class=comment>// Implementation -------------------------------------------------------------</span>
struct AudioEvents
{
	volatile bool quit = false;
	HANDLE wakeAudioThreadEvent;
};

struct AudioNotifications : public IMMNotificationClient
{
	<span class=comment>// Boiler plate</span>
	ULONG STDMETHODCALLTYPE AddRef() { return 1; }
	ULONG STDMETHODCALLTYPE Release() { return 1; }
	HRESULT STDMETHODCALLTYPE QueryInterface(REFIID riid, VOID **ppvInterface) { return S_OK; }
	HRESULT OnDeviceAdded(LPCWSTR pwstrDeviceId) { return S_OK; }
	HRESULT OnDeviceRemoved(LPCWSTR pwstrDeviceId) { return S_OK; }
	HRESULT OnDeviceStateChanged(LPCWSTR pwstrDeviceId, DWORD dwNewState) { return S_OK; }
	HRESULT OnPropertyValueChanged(LPCWSTR pwstrDeviceId, const PROPERTYKEY key) { return S_OK; }

	<span class=comment>// Parts that matter</span>
	volatile bool deviceChanged = false;
	HANDLE wakeAudioThreadEvent;

	HRESULT OnDefaultDeviceChanged(EDataFlow flow, ERole role, LPCWSTR pwstrDefaultDeviceId) {
		if (flow == eRender && role == eMultimedia) {
			deviceChanged = true;
			SetEvent(wakeAudioThreadEvent);
		}
		return S_OK;
	}
};

struct AudioDevice
{
	uint32_t bufferSampleCount;
	AudioNotifications* notifications;
	IMMDeviceEnumerator* enumerator;
	IMMDevice* endpoint;
	IAudioClient* audioClient;
	IAudioRenderClient* renderClient;
};

struct AudioBuffer
{
	float* buffer;
	uint32_t sampleCount;
	uint32_t elapsedSampleTime; <span class=comment>// Same as sampleCount when a device is connected</span>
};

AudioEvents createAudioEvents()
{
	AudioEvents events = {0};
	events.wakeAudioThreadEvent = CreateEventA(0, false, false, 0);
	return events;
}

void endAudioEvents(AudioEvents* events)
{
	events-&gt;quit = true;
	SetEvent(events-&gt;wakeAudioThreadEvent);
}

void destroyAudioEvents(AudioEvents* events)
{
	CloseHandle(events-&gt;wakeAudioThreadEvent);
}

AudioDevice createAudioDevice(AudioEvents* events)
{
	AudioDevice device = {0};

	CoCreateInstance( __uuidof(MMDeviceEnumerator), NULL, CLSCTX_ALL, __uuidof(IMMDeviceEnumerator), (void**)&device.enumerator);
	device.notifications = new AudioNotifications;
	device.notifications-&gt;wakeAudioThreadEvent = events-&gt;wakeAudioThreadEvent;
	device.enumerator-&gt;RegisterEndpointNotificationCallback(device.notifications);

	if (SUCCEEDED(device.enumerator-&gt;GetDefaultAudioEndpoint(eRender, eMultimedia, &device.endpoint))) {
		if (SUCCEEDED(device.endpoint-&gt;Activate(__uuidof(IAudioClient), CLSCTX_ALL, NULL, (void**)&device.audioClient))) {
			REFERENCE_TIME defaultDevicePeriod;
			REFERENCE_TIME minDevicePeriod;
			device.audioClient-&gt;GetDevicePeriod(&defaultDevicePeriod, &minDevicePeriod);

			WAVEFORMATEX customFormat = {0};
			customFormat.wFormatTag = WAVE_FORMAT_IEEE_FLOAT;
			customFormat.nChannels = CHANNELS_PER_SAMPLE;
			customFormat.nSamplesPerSec = SAMPLES_PER_SECOND;
			customFormat.wBitsPerSample = BYTES_PER_CHANNEL*8;
			customFormat.nBlockAlign = (CHANNELS_PER_SAMPLE*BYTES_PER_CHANNEL);
			customFormat.nAvgBytesPerSec = SAMPLES_PER_SECOND*customFormat.nBlockAlign;
			device.audioClient-&gt;Initialize(AUDCLNT_SHAREMODE_SHARED, AUDCLNT_STREAMFLAGS_EVENTCALLBACK | AUDCLNT_STREAMFLAGS_AUTOCONVERTPCM, defaultDevicePeriod, 0, &customFormat, 0);
			device.audioClient-&gt;SetEventHandle(events-&gt;wakeAudioThreadEvent);
			device.audioClient-&gt;GetService(__uuidof(IAudioRenderClient), (void**)&device.renderClient);
			device.audioClient-&gt;GetBufferSize(&device.bufferSampleCount);
			device.audioClient-&gt;Start();
		}
	}

	return device;
}

void destroyAudioDevice(AudioDevice* device)
{
	if (device-&gt;enumerator) {
		device-&gt;enumerator-&gt;UnregisterEndpointNotificationCallback(device-&gt;notifications);
		device-&gt;enumerator-&gt;Release();
	}
	if (device-&gt;notifications) delete device-&gt;notifications;
	if (device-&gt;renderClient) device-&gt;renderClient-&gt;Release();
	if (device-&gt;audioClient) device-&gt;audioClient-&gt;Release();
	if (device-&gt;endpoint) device-&gt;endpoint-&gt;Release();
}

bool waitForAudioBuffer(AudioDevice* device, AudioEvents* events, AudioBuffer* out_buffer)
{
	DWORD waitResult = WaitForSingleObject(events-&gt;wakeAudioThreadEvent, 1000);
	if (events-&gt;quit) return false;

	if (device-&gt;notifications-&gt;deviceChanged) {
		destroyAudioDevice(device);
		*device = createAudioDevice(events);
	}

	memset(out_buffer, 0, sizeof(AudioBuffer));
	if (device-&gt;renderClient) {
		uint32_t paddingSampleCount; 
		device-&gt;audioClient-&gt;GetCurrentPadding(&paddingSampleCount);
		uint32_t sampleCount = device-&gt;bufferSampleCount - paddingSampleCount;
		float* buffer;
		if (SUCCEEDED(device-&gt;renderClient-&gt;GetBuffer(sampleCount, (BYTE**)&buffer))) {
			out_buffer-&gt;buffer = buffer;
			out_buffer-&gt;sampleCount = sampleCount;
			out_buffer-&gt;elapsedSampleTime = sampleCount;
			memset(out_buffer-&gt;buffer, 0, BYTES_PER_CHANNEL*CHANNELS_PER_SAMPLE*sampleCount);
		}
	}
	else if (waitResult == WAIT_TIMEOUT) {
		out_buffer-&gt;elapsedSampleTime = SAMPLES_PER_SECOND;
	}

	return true;
}

void submitAudioBuffer(AudioDevice* device, AudioBuffer buffer)
{
	if (device-&gt;renderClient) device-&gt;renderClient-&gt;ReleaseBuffer(buffer.sampleCount, 0);
}

<span class=comment>// Example Program -------------------------------------------------------------</span>
struct AudioThreadData
{
	AudioEvents events;
	char sound;
};

DWORD WINAPI audioThread(void* param)
{
	AudioThreadData* data = (AudioThreadData*)param;

	AudioDevice device = createAudioDevice(&data-&gt;events);
	float t = 0;
	float volume = 1;
	float toneHerz = 0;

	AudioBuffer buffer = {0};
	while (waitForAudioBuffer(&device, &data-&gt;events, &buffer)) {
		<span class=comment>// Sine wave</span>
		for (uint32_t i=0; i&lt;buffer.sampleCount; ++i) {
			// Smoothly turn audio on and off so there's not clicks
			if (data-&gt;sound == ' ') {
				volume = max(0, volume - 0.01f);
			}
			else {
				toneHerz = data-&gt;sound * data-&gt;sound / 8;
				volume = min(1, volume + 0.01f);
			}
			<span class=comment>// Render the sound</span>
			buffer.buffer[i] = volume * sinf(t*3.14159f/SAMPLES_PER_SECOND);
			t += toneHerz;
		}
		submitAudioBuffer(&device, buffer);
	}
	destroyAudioDevice(&device);
	return 0;
}

int main(int argc, char** argv)
{
	CoInitializeEx(nullptr, COINIT_MULTITHREADED | COINIT_DISABLE_OLE1DDE);
	AudioThreadData threadData = {0};
	threadData.events = createAudioEvents();
	HANDLE threadHandle = CreateThread(0, 0, audioThread, &threadData, 0, 0);

	for (int i=0; i&lt;strlen(argv[1]); ++i) {
		threadData.sound = argv[1][i];
		Sleep(150);
	}

	endAudioEvents(&threadData.events);
	WaitForSingleObject(threadHandle, INFINITE);
	destroyAudioEvents(&threadData.events);
	return 0;
}</pre>
			</details>
		</div>
	</div>
</body>
</html>